rpart_fit <- rpart(median_house_value ~ latitude + longitude + housing_median_age +
total_rooms + total_bedrooms + population + median_income +
ocean_proximity, data = df_train)
rpart_predictions <- predict(rpart_fit, newdata = df_test)
rpart.plot(rpart_fit)
rmse_rpart <- sqrt(mean((df_test$median_house_value - rpart_predictions)^2))
rmse_rpart
library(e1071)
svm_fit <- svm(median_house_value ~ latitude + longitude + housing_median_age +
total_rooms + total_bedrooms + population + median_income +
ocean_proximity, data = df_train, kernel = "radial")
svm_predictions <- predict(svm_fit, newdata = df_test)
rmse_svm <- sqrt(mean((df_test$median_house_value - svm_predictions)^2))
rmse_svm
library(torch)
library(luz)
# 初始化一个神经网络模型的架构
NNet <- nn_module(
initialize = function(p, q1, q2, q3){
self$layer1 <- nn_linear(p, q1)
self$layer2 <- nn_linear(q1, q2)
self$layer3 <- nn_linear(q2, q3)
self$layer4 <- nn_linear(q3, 1)
},
forward = function(x){
x %>%
self$layer1() %>%
nnf_relu() %>%
self$layer2() %>%
nnf_relu() %>%
self$layer3() %>%
nnf_relu() %>%
self$layer4()
}
)
library(torch)
library(luz)
# 初始化神经网络模型架构
NNet <- nn_module(
initialize = function(p, q1, q2, q3){
self$layer1 <- nn_linear(p, q1)
self$layer2 <- nn_linear(q1, q2)
self$layer3 <- nn_linear(q2, q3)
self$output <- nn_linear(q3, 1)
},
forward = function(x){
x %>%
self$layer1() %>%
nnf_relu() %>%
self$layer2() %>%
nnf_relu() %>%
self$layer3() %>%
nnf_relu() %>%
self$output()
}
)
# 生成协变量矩阵
covariates <- model.matrix(~ . - 1, data = df_train)
target <- df_train$median_house_value
# 转换为 torch 张量
covariates <- torch_tensor(as.matrix(covariates), dtype = torch_float32)
library(torch)
library(luz)
# 初始化神经网络模型架构
NNet <- nn_module(
initialize = function(p, q1, q2, q3){
self$layer1 <- nn_linear(p, q1)
self$layer2 <- nn_linear(q1, q2)
self$layer3 <- nn_linear(q2, q3)
self$output <- nn_linear(q3, 1)
},
forward = function(x){
x %>%
self$layer1() %>%
nnf_relu() %>%
self$layer2() %>%
nnf_relu() %>%
self$layer3() %>%
nnf_relu() %>%
self$output()
}
)
covariates <- as.matrix(model.matrix(~ . - 1, data = df_train))
target <- as.matrix(df_train$median_house_value)
# 创建 torch 张量
covariates_tensor <- torch_tensor(covariates, dtype = torch_float32)
remove.packages("torch")
install.packages("torch")
library(torch)
install.packages("torch")
torch::install_torch()
library(torch)
library(luz)
# 初始化神经网络模型架构
NNet <- nn_module(
initialize = function(p, q1, q2, q3){
self$layer1 <- nn_linear(p, q1)
self$layer2 <- nn_linear(q1, q2)
self$layer3 <- nn_linear(q2, q3)
self$output <- nn_linear(q3, 1)
},
forward = function(x){
x %>%
self$layer1() %>%
nnf_relu() %>%
self$layer2() %>%
nnf_relu() %>%
self$layer3() %>%
nnf_relu() %>%
self$output()
}
)
covariates <- as.matrix(model.matrix(~ . - 1, data = df_train))
target <- as.matrix(df_train$median_house_value)
# 创建 torch 张量
covariates_tensor <- torch_tensor(covariates, dtype = torch_float32)
library(torch)
library(luz)
# 定义一个具有三个隐藏层的神经网络结构
NNet <- nn_module(
initialize = function(p, q1, q2, q3){
self$layer1 <- nn_linear(p, q1)
self$layer2 <- nn_linear(q1, q2)
self$layer3 <- nn_linear(q2, q3)
self$output <- nn_linear(q3, 1)
},
forward = function(x){
x %>%
self$layer1() %>%
nnf_relu() %>%
self$layer2() %>%
nnf_relu() %>%
self$layer3() %>%
nnf_relu() %>%
self$output()
}
)
# 准备数据
covariates <- as.matrix(model.matrix(~ . - 1, data = df_train))
target <- df_train$median_house_value
# 转换为torch张量
covariates_tensor <- torch_tensor(covariates, dtype = torch_float32)
library(torch)
library(luz)
# 定义一个具有三个隐藏层的神经网络结构
NNet <- nn_module(
initialize = function(p, q1, q2, q3){
self$layer1 <- nn_linear(p, q1)
self$layer2 <- nn_linear(q1, q2)
self$layer3 <- nn_linear(q2, q3)
self$output <- nn_linear(q3, 1)
},
forward = function(x){
x %>%
self$layer1() %>%
nnf_relu() %>%
self$layer2() %>%
nnf_relu() %>%
self$layer3() %>%
nnf_relu() %>%
self$output()
}
)
# 准备数据
covariates <- as.matrix(model.matrix(~ . - 1, data = df_train))
target <- df_train$median_house_value
# 设置神经网络并拟合模型
nnet_fit <- NNet %>%
setup(
loss = nnf_mse_loss,
optimizer = optim_adam,
metrics = list(
rmse = function(y, yhat) { sqrt(mean((y - yhat)^2)) }
)
) %>%
set_hparams(layer1 = 32, layer2 = 16, layer3 = 8) %>%
set_opt_params(lr = 0.01) %>%
fit(
x = covariates_tensor,
y = target_tensor,
epochs = 100,
batch_size = 64,
dataloader_options = list(shuffle = TRUE)
)
library(torch)
library(luz)
# 定义一个具有三个隐藏层的神经网络结构
NNet <- nn_module(
initialize = function(p, q1, q2, q3){
self$layer1 <- nn_linear(p, q1)
self$layer2 <- nn_linear(q1, q2)
self$layer3 <- nn_linear(q2, q3)
self$output <- nn_linear(q3, 1)
},
forward = function(x){
x %>%
self$layer1() %>%
nnf_relu() %>%
self$layer2() %>%
nnf_relu() %>%
self$layer3() %>%
nnf_relu() %>%
self$output()
}
)
# 准备数据
covariates <- as.matrix(model.matrix(~ . - 1, data = df_train))
target <- df_train$median_house_value
# 转换为torch张量
covariates_tensor <- torch_tensor(covariates, dtype = torch_float32)
library(torch)
library(luz)
# 定义一个具有三个隐藏层的神经网络结构
NNet <- nn_module(
initialize = function(p, q1, q2, q3){
self$layer1 <- nn_linear(p, q1)
self$layer2 <- nn_linear(q1, q2)
self$layer3 <- nn_linear(q2, q3)
self$output <- nn_linear(q3, 1)
},
forward = function(x){
x %>%
self$layer1() %>%
nnf_relu() %>%
self$layer2() %>%
nnf_relu() %>%
self$layer3() %>%
nnf_relu() %>%
self$output()
}
)
# 准备数据
covariates <- as.matrix(model.matrix(~ . - 1, data = df_train))
target <- df_train$median_house_value
# 转换数据为数值矩阵
covariates_matrix <- as.matrix(covariates)
target_vector <- as.numeric(target)
# 创建torch张量
covariates_tensor <- torch_tensor(covariates_matrix, dtype = torch_float32)
library(torch)
library(luz)
# 定义一个具有三个隐藏层的神经网络结构
NNet <- nn_module(
initialize = function(p, q1, q2, q3){
self$layer1 <- nn_linear(p, q1)
self$layer2 <- nn_linear(q1, q2)
self$layer3 <- nn_linear(q2, q3)
self$output <- nn_linear(q3, 1)
},
forward = function(x){
x %>%
self$layer1() %>%
nnf_relu() %>%
self$layer2() %>%
nnf_relu() %>%
self$layer3() %>%
nnf_relu() %>%
self$output()
}
)
# 准备数据
covariates <- as.matrix(model.matrix(~ . - 1, data = df_train))
target <- df_train$median_house_value
# 转换数据为数值矩阵
covariates_matrix <- as.matrix(covariates)
target_vector <- as.numeric(target)
# 创建torch张量
covariates_tensor <- torch_tensor(covariates_matrix, dtype = torch_float32)
library(torch)
library(luz)
# 定义一个具有三个隐藏层的神经网络结构
NNet <- nn_module(
initialize = function(p, q1, q2, q3){
self$layer1 <- nn_linear(p, q1)
self$layer2 <- nn_linear(q1, q2)
self$layer3 <- nn_linear(q2, q3)
self$output <- nn_linear(q3, 1)
},
forward = function(x){
x %>%
self$layer1() %>%
nnf_relu() %>%
self$layer2() %>%
nnf_relu() %>%
self$layer3() %>%
nnf_relu() %>%
self$output()
}
)
# 准备数据
covariates <- as.matrix(model.matrix(~ . - 1, data = df_train))
target <- df_train$median_house_value
# 转换数据为数值矩阵
covariates_matrix <- as.matrix(covariates)
target_vector <- as.numeric(target)
# 创建torch张量
covariates_tensor <- torch_tensor(covariates_matrix, dtype = torch_float32)
library(rpart)
library(rpart.plot)
rpart_fit <- rpart(spam ~ ., data = df_train, method = "class")
library(rpart)
library(rpart.plot)
rpart_fit <- rpart(spam ~ ., data = df_train, method = "class")
packages <- c(
"tibble",
"dplyr",
"readr",
"tidyr",
"purrr",
"broom",
"magrittr",
"corrplot",
"caret",
"rpart",
"rpart.plot",
"e1071",
"torch",
"luz"
)
# renv::install(packages)
sapply(packages, require, character.only=T)
library(tibble)
library(dplyr)
# read dat6a
path <- "data/housing.csv"
df <- read_csv(path) %>%
as_tibble() %>%
mutate(across(where(is.character), as.factor)) %>%
rename_with(tolower, everything()) %>%
drop_na()
glimpse(df)
library(corrplot)
df %>%
select_if(is.numeric) %>%
cor() %>%
corrplot()
set.seed(42)
test_ind <- sample(
1:nrow(df),
floor( nrow(df) / 10 ),
replace=FALSE
)
df_train <- df[-test_ind, ]
df_test <- df[test_ind, ]
library(dplyr)
library(tidyr)
library(modelr)
lm_fit <- lm(median_house_value ~ latitude + longitude + housing_median_age +
total_rooms + total_bedrooms + population + median_income +
ocean_proximity, data = df)
summary(lm_fit)
# define
rmse <- function(y, yhat) {
sqrt(mean((y - yhat)^2))
}
# predict
lm_predictions <- predict(lm_fit, newdata = df_test)
# calculate
rmse_value <- rmse(df_test$median_house_value, lm_predictions)
# output
rmse_value
library(rpart)
library(rpart.plot)
rpart_fit <- rpart(median_house_value ~ latitude + longitude + housing_median_age +
total_rooms + total_bedrooms + population + median_income +
ocean_proximity, data = df_train)
rpart_predictions <- predict(rpart_fit, newdata = df_test)
rpart.plot(rpart_fit)
rmse_rpart <- sqrt(mean((df_test$median_house_value - rpart_predictions)^2))
rmse_rpart
library(e1071)
svm_fit <- svm(median_house_value ~ latitude + longitude + housing_median_age +
total_rooms + total_bedrooms + population + median_income +
ocean_proximity, data = df_train, kernel = "radial")
svm_predictions <- predict(svm_fit, newdata = df_test)
rmse_svm <- sqrt(mean((df_test$median_house_value - svm_predictions)^2))
rmse_svm
NNet <- nn_module(
initialize = function(p, q1, q2, q3){
self$layer1 <- nn_linear(p, q1)
self$layer2 <- nn_linear(q1, q2)
self$layer3 <- nn_linear(q2, q3)
},
forward = function(x){
x <- self$layer1(x)
x <- nnf_relu(x)
x <- self$layer2(x)
x <- nnf_relu(x)
x <- self$layer3(x)
return(x)
}
)
library(luz)  # Ensure luz package is loaded
# Assuming your dataset is ready and NNet is your defined neural network model structure
nnet_fit <- NNet %>%
setup(
loss = nn_mse_loss(),  # Mean Squared Error Loss for regression problems
optimizer = optim_adam(lr = 0.001),  # Adam optimizer with a learning rate of 0.001
metrics = list(mae = luz_metric_mean_absolute_error())  # Mean Absolute Error as an additional metric
) %>%
set_hparams(
batch_size = 64,  # Batch size for training
epochs = 100      # Number of epochs for training
) %>%
set_opt_params(
lr = 0.001  # Learning rate, same as the one defined in the optimizer
) %>%
fit(
x = training_data,  # The features of your training dataset
y = training_targets,  # The target (median_house_value) of your training dataset
dataloader_options = list(batch_size = 64, shuffle = TRUE),  # DataLoader options such as batch size and shuffle
verbose = FALSE  # Set to TRUE for output during training, FALSE for final submission
)
set.seed(42)
test_ind <- sample(
1:nrow(df),
floor( nrow(df)/10 ),
replace=FALSE
)
df_train <- df[-test_ind, ]
df_test <- df[test_ind, ]
overview <- function(pred_class, true_class) {
accuracy <- mean(pred_class == true_class)
error <- mean(pred_class != true_class)
true_positives <- sum(true_class == "positive" & pred_class == "positive")
true_negatives <- sum(true_class == "negative" & pred_class == "negative")
false_positives <- sum(true_class == "negative" & pred_class == "positive")
false_negatives <- sum(true_class == "positive" & pred_class == "negative")
true_positive_rate <- true_positives / (true_positives + false_negatives)
false_positive_rate <- false_positives / (true_negatives + false_positives)
return(
data.frame(
accuracy = accuracy,
error = error,
true_positive_rate = true_positive_rate,
false_positive_rate = false_positive_rate
)
)
}
library(rpart)
library(rpart.plot)
rpart_fit <- rpart(spam ~ ., data = df_train, method = "class")
library(rpart)
library(rpart.plot)
# 确保df_train包含spam列
print(colnames(df_train))
# 拟合决策树模型，注意替换df_train和df_test为你实际使用的数据集变量名
rpart_fit <- rpart(spam ~ ., data = df_train, method = "class")
str(your_data)
library(rpart)
# 假设你的训练数据集是 df_train，其中 'spam' 是你需要预测的列
rpart_model <- rpart(spam ~ ., data=df_train, method="class")
library(rpart)
# 假设你的训练数据集是 df_train，其中 'spam' 是你需要预测的列
rpart_model <- rpart(spam ~ ., data=df_train, method="class")
# 查看 df_test 数据集中的所有列名
column_names <- names(df_test)
print(column_names)
library(tidyverse)
# 读取数据
path <- "data/spambase.csv"
df <- read_csv(path) %>%
mutate(across(everything(), as.factor)) %>% # 将所有变量转换为因子类型
rename_with(tolower) %>% # 列名转换为小写
drop_na() # 删除包含缺失值的观察值
# 显示处理后的数据
head(df)
set.seed(42)
test_ind <- sample(
1:nrow(df),
floor( nrow(df)/10 ),
replace=FALSE
)
df_train <- df[-test_ind, ]
df_test <- df[test_ind, ]
names()
names()
library(rpart)
# 假设你的训练数据集是 df_train，其中 'spam' 是你需要预测的列
rpart_model <- rpart(spam ~ ., data=df_train, method="class")
library(rpart.plot)
rpart.plot(rpart_model)
# 假设你的测试数据集是 df_test
predictions <- predict(rpart_model, newdata=df_test, type="class")
accuracy <- sum(predictions == df_test$spam) / nrow(df_test)
print(accuracy)
install.packages("e1071")
library(e1071)
# 假设你的训练数据集是 df_train，其中 'spam' 是你需要预测的列，并且已经是因子类型
svm_fit <- svm(spam ~ ., data=df_train, type="C-classification", kernel="radial")
library(e1071)
# 假设你的训练数据集是 df_train，其中 'spam' 是你需要预测的列，并且已经是因子类型
svm_fit <- svm(spam ~ ., data=df_train, type="C-classification", kernel="radial")
# 假设你的测试数据集是 df_test
svm_predictions <- predict(svm_fit, newdata=df_test)
svm_accuracy <- mean(svm_predictions == df_test$spam)
print(svm_accuracy)
library(torch)
# 准备数据
df_train <- your_data_frame_name  # 替换为你的数据框名称
library(torch)
# 准备数据
df_train <- your_data_frame_name  # 替换为你的数据框名称
library(torch)
# 准备数据
df_train <- df  # 替换为你的数据框名称
target <- df_train$spam  # 替换为你的目标变量列名
df_train$spam <- NULL  # 删除目标变量列，以便只有预测变量保留
# 转换为 tensor
input_tensor <- torch_tensor(as.matrix(df_train), dtype=torch_float32())
library(e1071)
svm_fit <- svm(spam ~ ., data=df_train, type="C-classification", kernel="radial")
